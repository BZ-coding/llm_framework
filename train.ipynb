{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ce3a5a-ab97-419c-a1e4-9859012a735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b6cefe-a2a0-411d-96ce-199b63da6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, get_peft_model_state_dict\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ae505a-500f-4b3f-8f1b-45951abc427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = '/mnt/nfs/zsd_server/models/huggingface/llama-7b-hf_yahma'\n",
    "DATA_PATH = '/mnt/nfs/zsd_server/data/origin/alpaca_data.json'\n",
    "SAVE_PATH = '/mnt/nfs/zsd_server/models/my/llama-7b_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4890f2dc-cf11-4e3e-8b85-0f97ea12fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    max_length = 512\n",
    "    micro_batch_size = 1\n",
    "    gradient_accumulation_steps = 64\n",
    "    batch_size = micro_batch_size * gradient_accumulation_steps\n",
    "    learning_rate = 3e-4\n",
    "    epoch = 1\n",
    "    train_steps = 52000 // batch_size * epoch\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.05\n",
    "    lora_target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9573218a-dee6-434a-8766-1a583e7ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f335aa1-873e-4211-ae53-b817575bf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "# The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = logging.INFO\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de56f1be-7631-4952-8778-4e28c2bd546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:713] 2023-12-16 01:28:55,004 >> loading configuration file /mnt/nfs/zsd_server/models/huggingface/llama-7b-hf_yahma/config.json\n",
      "[INFO|configuration_utils.py:775] 2023-12-16 01:28:55,005 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2776] 2023-12-16 01:28:55,006 >> loading weights file /mnt/nfs/zsd_server/models/huggingface/llama-7b-hf_yahma/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1191] 2023-12-16 01:28:55,006 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2023-12-16 01:28:55,007 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772a1127e7fd4fc1a2726b2a943d5bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3551] 2023-12-16 01:28:59,775 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3559] 2023-12-16 01:28:59,776 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mnt/nfs/zsd_server/models/huggingface/llama-7b-hf_yahma.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:728] 2023-12-16 01:28:59,778 >> loading configuration file /mnt/nfs/zsd_server/models/huggingface/llama-7b-hf_yahma/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2023-12-16 01:28:59,778 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    # load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=\"auto\",\n",
    "    # device_map=\"cpu\",\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1708cc-f3b6-478f-81b2-72453117021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1850] 2023-12-16 01:28:59,788 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-12-16 01:28:59,788 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-12-16 01:28:59,788 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-12-16 01:28:59,788 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:305] 2023-12-16 01:28:59,789 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id:0 tokenizer.pad_token:<unk>\n",
      "tokenizer.bos_token_id:1 tokenizer.bos_token:<s>\n",
      "tokenizer.eos_token_id:2 tokenizer.eos_token:</s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "# tokenizer = LlamaTokenizerFast.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "# tokenizer.bos_token_id = 1  # <s>\n",
    "# tokenizer.eos_token_id = 2  # </s>\n",
    "# print(tokenizer)\n",
    "print(f\"tokenizer.pad_token_id:{tokenizer.pad_token_id} tokenizer.pad_token:{tokenizer.pad_token}\")\n",
    "print(f\"tokenizer.bos_token_id:{tokenizer.bos_token_id} tokenizer.bos_token:{tokenizer.bos_token}\")\n",
    "print(f\"tokenizer.eos_token_id:{tokenizer.eos_token_id} tokenizer.eos_token:{tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a175c5d1-647d-4112-ba71-74bd2ea64fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3b046fe65c5aa960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.builder - Using custom data configuration default-3b046fe65c5aa960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/zsd/miniconda3/envs/huggingface/lib/python3.11/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.info - Loading Dataset Infos from /home/zsd/miniconda3/envs/huggingface/lib/python3.11/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.info - Loading Dataset info from /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.builder - Found cached dataset json (/home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.info - Loading Dataset info from /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Dataset({\n",
      "    features: ['instruction', 'output', 'input'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "# print(data)\n",
    "print(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df0954d-9bdd-449e-93fa-8f9b71f17aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{data_point['instruction']}\"\"\"\n",
    "\n",
    "    if data_point['input']:\n",
    "        prompt += f\"\"\"\\n\\n### Input:\\n{data_point['input']}\"\"\"\n",
    "\n",
    "    prompt += f\"\"\"\\n\\n### Response:\\n{data_point['output']}\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f7d725-33d6-4354-a859-dd3cd3c268c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=ARGS.max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < ARGS.max_length\n",
    "            and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4473ab-efb1-4add-a7ee-16691685f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae966ab5-2a3d-403b-ad67-efba296e4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 12199, 13, 13, 2277, 29937, 13291, 29901, 13, 10994, 29889, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 12199, 13, 13, 2277, 29937, 13291, 29901, 13, 10994, 29889, 2]}\n",
      "<class 'list'>\n",
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "hello\n",
      "\n",
      "### Response:\n",
      "Hello.</s>\n"
     ]
    }
   ],
   "source": [
    "a = generate_and_tokenize_prompt({'instruction': 'hello', 'input': '', 'output': 'Hello.'})\n",
    "print(a)\n",
    "print(type(a['input_ids']))\n",
    "print(tokenizer.decode(a['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a900cc-cc8e-4097-a54c-ef9371c7fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ca01b4a35da0554.arrow and /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8880a9c7aab019ed.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.arrow_dataset - Loading cached split indices for dataset at /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ca01b4a35da0554.arrow and /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8880a9c7aab019ed.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c3bcce630c169df7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c3bcce630c169df7.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b5e56414dbfe2789.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16/2023 01:29:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zsd/.cache/huggingface/datasets/json/default-3b046fe65c5aa960/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b5e56414dbfe2789.arrow\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 51802\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = data[\"train\"].train_test_split(test_size=200, shuffle=True, seed=42)\n",
    "data[\"train\"] = data[\"train\"].map(generate_and_tokenize_prompt, remove_columns=data[\"train\"].column_names)\n",
    "data[\"test\"] = data[\"test\"].map(generate_and_tokenize_prompt, remove_columns=data[\"test\"].column_names)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a72a33-a336-4e72-917c-7dcbb13beb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,392,576 || trainable%: 0.589770503135875\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=ARGS.lora_r,\n",
    "    lora_alpha=ARGS.lora_alpha,\n",
    "    target_modules=ARGS.lora_target_modules,\n",
    "    lora_dropout=ARGS.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af5ea01-387c-4c16-af88-0c8da8be6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22531a1f-6675-4ed9-8dde-3ad6d7169932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1327] 2023-12-16 01:29:01,049 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1769] 2023-12-16 01:29:01,049 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1480] 2023-12-16 01:29:01,049 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=ARGS.micro_batch_size,\n",
    "    gradient_accumulation_steps=ARGS.gradient_accumulation_steps,\n",
    "    # warmup_steps=100,\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=ARGS.train_steps,\n",
    "    learning_rate=ARGS.learning_rate,\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    output_dir=SAVE_PATH,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    # report_to=\"tensorboard\",\n",
    "    logging_dir='logs',\n",
    "    logging_steps=1,\n",
    "    auto_find_batch_size=False,\n",
    "    # torch_compile=True,\n",
    "    do_train=True,\n",
    ")\n",
    "# training_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f40b0fea-a65f-46d7-b106-20c85883a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8, \n",
    "    # pad_to_multiple_of=ARGS.max_length, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36c1ff19-b36a-4674-a8ff-f6bffdebc205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=LlamaTokenizer(name_or_path='/mnt/nfs/zsd_server/models/huggingface/llama-7b-hf_yahma', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False), model=None, padding=True, max_length=None, pad_to_multiple_of=8, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18c5968c-4044-4ed4-af42-f3b19303c560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([data[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15bc1d19-7cad-4092-8279-5c560e08559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dde4e77b-967a-487e-83d6-ccc64dcd4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c6f7545-ab29-45e0-93b6-ece2160ea0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dd77e5e-f799-4407-b1c8-4dc5a4e8385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = transformers.Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=data[\"train\"],\n",
    "#     eval_dataset=data[\"test\"],\n",
    "#     args=training_arguments,\n",
    "#     data_collator=data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4013954d-95a2-436c-bd8c-c7f65b9877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27777948-5996-4fa3-a7ab-b36d6d999ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e9dcfeb-4239-4b3a-a2f0-f1c889fd801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:393] 2023-12-16 01:29:01,091 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:565] 2023-12-16 01:29:01,092 >> max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d538e420-d6ed-4390-864c-896fa7d2f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56358c1-7935-4606-bcf2-687dc363eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1714] 2023-12-16 01:29:01,164 >> ***** Running training *****\n",
      "[INFO|trainer.py:1715] 2023-12-16 01:29:01,165 >>   Num examples = 51,802\n",
      "[INFO|trainer.py:1716] 2023-12-16 01:29:01,165 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1717] 2023-12-16 01:29:01,165 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1720] 2023-12-16 01:29:01,165 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1721] 2023-12-16 01:29:01,165 >>   Gradient Accumulation steps = 64\n",
      "[INFO|trainer.py:1722] 2023-12-16 01:29:01,166 >>   Total optimization steps = 812\n",
      "[INFO|trainer.py:1723] 2023-12-16 01:29:01,167 >>   Number of trainable parameters = 39,976,960\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18/812 02:02 < 1:41:32, 0.13 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b39cc-4b23-4038-8e71-0a31b1b8ab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a4269-0746-4771-8e5b-c35a41890e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
